# test_task

## Содержание директорий
- dataset &mdash; функции и классы для работы с датасетом
- experiment &mdash; непосредственно сам эксперимент по классфикации TinyImageNet
- models &mdash; кастомизируемая модель, используемая для классификации
- results &mdash; результаты: чекпоинты для TensorBoard, confusion matrix
- utils &mdash; функции для обучения и валидации, необходимые пути
- visualization &mdash; функции для визуализации
- main.py &mdash; конфигурация для эксперимента
- run.sh &mdash; скрипт для запуска

## Что было протестировано в экспериментах:
1) Различные feature extractor'ы (backbone): ResNet, WideResNet, EfficientNet. В итоге наилучший результат был получен с использованием модели EfficientNet, что не удивительно, так как у нее самое лучшее качество на ImageNet (брал предобученные на ImageNet модели). На моделях из семейства ResNet и WideResNet было получено качество около 60% (а именно, на моделях с большим количеством слоев). EfficientNet дала результат 63,13% (такой результат был получен на версии B5, для версий B6 и B7 не получалось поставить батчсайз больший, чем 128, поэтому даже такое качество на последних версиях EfficientNet не получилось воспроизвести)
2) Различные оптимизаторы: Adam и SGD. Для SGD не получилось подобрать гиперпараметры такие, чтобы он превосходил по качеству Adam.
3) Были добавлены аугментации. Очевидно, они помогают в некоторой степени избежать переобучения и "дают" больше данных для обучения. 
4) Была протестирована регуляризация весов модели, но прироста качества она не дала, вероятно, потому, что модель не сильно переобучается на исходных данных.
5) Также были рассмотрены несколько вариантов головы-классификатора, состоящей из полносвязных слоев и активаций. Как показали эксперименты, лучше взять голову чуть побольше, чем просто один слой для итоговой классификации.
6) Различный батчсайз. Для EfficientNet не было достаточно батчсайза, равного 128, поэтому лучший результат был получен на версии B5, для которой получилось поставить батчсайз 256. В остальном, батчсайз, больший 256, почти не влиял не результат и скорость сходимости. Но для меньших его значений уменьшалась скорость сходимости и итоговое качество, так как чем больше "знаний" (объектов в батче) есть для градиентного спуска, тем точнее можно предсказать поведение минимизируемой функции.
7) Шедулинг для learning rate, а именно, ReduceLROnPlateau. Он помог повысить качество на несколько процентов, так как с константным learning rate в некоторый момент градиентный спуск не может найти необходимый минимум функционала, потому что шаг производится очень грубо (с большим learning rate градиентный спуск "шагает" далеко).

## Графики

Лосс на обучающей выборке | Лосс на тестовой выборке
:-------------------------:|:-------------------------:
![](/results/plots/train_loss.png) | ![](/results/plots/test_loss.png)

Accuracy на обучающей выборке | Accuracy на тестовой выборке
:-------------------------:|:-------------------------:
![](/results/plots/train_accuracy.png) | ![](/results/plots/test_accuracy.png)

Как видно из графиков, и лосс, и accuracy стабилизировались. Это свидетельствует о том, что обучение нейросети сошлось к некоторому локальному оптимуму.

## Confusion matrix

В начале обучения (после 1-ой эпохи) | В конце обучения (после 100-ой эпохи)
:-------------------------:|:-------------------------:
![](/results/confusion_matrix/953_0.png) | ![](/results/confusion_matrix/953_99.png)

Видно, что в начале обучения было гораздо больше ошибок, чем в конце (много ярких квадратиков не на диагонали на первой картинке). Но все-таки ошибки в конце обучения тоже присутствуют (эти квадратики не на диагонали уже видны гораздо хуже, но их достаточное количество).
